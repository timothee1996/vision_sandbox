{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog image generation using GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10, 10)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to stanford dogs image dataset path\n",
    "# available at http://vision.stanford.edu/aditya86/ImageNetDogs/\n",
    "DATASET_PATH = '/media/tguy/Records/timothee/workspace/research/datasets/generative-dog-images/all_dogs_imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw dataset\n",
    "original_ds = ImageFolder(\n",
    "    DATASET_PATH,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "original_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display grid of images\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(6, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[i, j].imshow(original_ds[i * 4 + j][0].permute(1, 2, 0))\n",
    "        axes[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_target_size = 64\n",
    "\n",
    "# preprocessing transform\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(img_target_size),\n",
    "    transforms.CenterCrop(img_target_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "train_ds = ImageFolder(DATASET_PATH, transform=preprocess)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GAN model with pytorch lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deconvolution_out_size(\n",
    "    in_size: int,\n",
    "    kernel_size: int,\n",
    "    stride: int = 1,\n",
    "    padding: int = 0,\n",
    "    output_padding: int = 0,\n",
    "    dilation: int = 1,\n",
    ") -> int:\n",
    "    return int(\n",
    "        (in_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_deconvolution_out_size(32, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        generated_img = self.model(x)\n",
    "        return generated_img\n",
    "\n",
    "\n",
    "# define discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        validity = self.model(x)\n",
    "        return validity.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAN class with LightningModule\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "\n",
    "\n",
    "class GAN_for_dogs(L.LightningModule):\n",
    "    def __init__(self,\n",
    "                 latent_dim: int = 100,\n",
    "                 lr_g: float = 0.001,\n",
    "                 lr_d: float = 0.0005,\n",
    "                 b1: float = 0.5,\n",
    "                 b2: float = 0.999,\n",
    "                 target_img_size: tuple[int, int] = (64, 64)) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr_g = lr_g\n",
    "        self.lr_d = lr_d\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.target_img_size = target_img_size\n",
    "        \n",
    "        # manual optimization\n",
    "        self.automatic_optimization=False\n",
    "\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.validation_z = torch.randn(8, self.latent_dim, 1, 1)\n",
    "        self.example_input_array = torch.zeros(2, self.latent_dim, 1, 1)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # generate image from random noise\n",
    "        return self.generator(z)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        imgs, _ = batch\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        \n",
    "        real_targets = torch.ones(batch_size, dtype=imgs.dtype).to(self.device)\n",
    "        fake_targets = torch.zeros(batch_size, dtype=imgs.dtype).to(self.device)\n",
    "        \n",
    "\n",
    "        # noise sampling\n",
    "        z = torch.randn(batch_size, self.latent_dim, 1, 1, dtype=imgs.dtype).to(self.device)\n",
    "        \n",
    "        # optimize on generator\n",
    "        generated = self.generator(z)\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            for i, genereted_img in enumerate(generated):\n",
    "                self.logger.experiment.add_image(f'generated/{i}', genereted_img * 0.5 + 0.5, self.global_step)\n",
    "                \n",
    "        \n",
    "        loss_g = self.criterion(self.discriminator(generated), real_targets)\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(loss_g)\n",
    "        opt_g.step()\n",
    "        \n",
    "        # discriminator\n",
    "        loss_d = (\n",
    "            self.criterion(self.discriminator(imgs), real_targets)\n",
    "            + self.criterion(self.discriminator(generated.detach()), fake_targets)\n",
    "        ) / 2\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(loss_d)\n",
    "        opt_d.step()\n",
    "        \n",
    "        \n",
    "        self.log_dict(\n",
    "            {\n",
    "                'loss_g': loss_g,\n",
    "                'loss_d': loss_d,\n",
    "            },\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        # when multiple optimizers are used, optimization is done manually in lightning\n",
    "        opt_g = optim.AdamW(\n",
    "            self.generator.parameters(),\n",
    "            lr=self.lr_g,\n",
    "            betas=(self.b1, self.b2)\n",
    "        )\n",
    "        opt_d = optim.AdamW(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.lr_d,\n",
    "            betas=(self.b1, self.b2)\n",
    "        )\n",
    "        \n",
    "        # TODO: test with ReduceLROnPlateau\n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    max_epochs=500,\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN_for_dogs()\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
